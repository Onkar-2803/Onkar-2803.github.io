---
---

@string{aps = {American Physical Society,}}

@inproceedings{litake-etal-2022-l3cube,
    title = "{L}3{C}ube-{M}aha{NER}: A {M}arathi Named Entity Recognition Dataset and {BERT} models",
    author = "Litake, Onkar  and
      Sabane, Maithili Ravindra  and
      Patil, Parth Sachin  and
      Ranade, Aparna Abhijeet  and
      Joshi, Raviraj",
    editor = "Jha, Girish Nath  and
      L., Sobha  and
      Bali, Kalika  and
      Ojha, Atul Kr.",
    booktitle = "Proceedings of the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.wildre-1.6",
    pages = "29--34",
    abstract = "Named Entity Recognition (NER) is a basic NLP task and finds major applications in conversational and search systems. It helps us identify key entities in a sentence used for the downstream application. NER or similar slot filling systems for popular languages have been heavily used in commercial applications. In this work, we focus on Marathi, an Indian language, spoken prominently by the people of Maharashtra state. Marathi is a low resource language and still lacks useful NER resources. We present L3Cube-MahaNER, the first major gold standard named entity recognition dataset in Marathi. We also describe the manual annotation guidelines followed during the process. In the end, we benchmark the dataset on different CNN, LSTM, and Transformer based models like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides the best performance among all the models. The data and models are available at \url{https://github.com/l3cube-pune/MarathiNLP} .",
    selected={true},
}

@inproceedings{litake2023mono,
  title={Mono versus multilingual bert: A case study in hindi and marathi named entity recognition},
  author={Litake, Onkar and Sabane, Maithili and Patil, Parth and Ranade, Aparna and Joshi, Raviraj},
  booktitle={Proceedings of 3rd International Conference on Recent Trends in Machine Learning, IoT, Smart Cities and Applications: ICMISC 2022},
  pages={607--618},
  year={2023},
  organization={Springer},
  abstract = "Named entity recognition (NER) is the process of recognising and classifying important information (entities) in text. Proper nouns, such as a person's name, an organization's name, or a location's name, are examples of entities. The NER is one of the important modules in applications like human resources, customer support, search engines, content classification, and academia. In this work, we consider NER for low-resource Indian languages like Hindi and Marathi. The transformer-based models have been widely used for NER tasks. We consider different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark them on publicly available Hindi and Marathi NER datasets. We provide an exhaustive comparison of different monolingual and multilingual transformer-based models and establish simple baselines currently missing in the literature. We show that the monolingual MahaRoBERTa model performs the best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for Hindi NER. We also perform cross-language evaluation and present mixed observations.",
  selected={true}
}

@inproceedings{vyawahare-etal-2022-pict,
    title = "{PICT}@{D}ravidian{L}ang{T}ech-{ACL}2022: Neural Machine Translation On {D}ravidian Languages",
    author = "Vyawahare, Aditya  and
      Tangsali, Rahul  and
      Mandke, Aditya  and
      Litake, Onkar  and
      Kadam, Dipali",
    editor = "Chakravarthi, Bharathi Raja  and
      Priyadharshini, Ruba  and
      Madasamy, Anand Kumar  and
      Krishnamurthy, Parameswari  and
      Sherly, Elizabeth  and
      Mahesan, Sinnathamby",
    booktitle = "Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dravidianlangtech-1.28",
    doi = "10.18653/v1/2022.dravidianlangtech-1.28",
    pages = "177--183",
    abstract = "This paper presents a summary of the findings that we obtained based on the shared task on machine translation of Dravidian languages. As a part of this shared task, we carried out neural machine translations for the following five language pairs: Kannada to Tamil, Kannada to Telugu, Kannada to Malayalam, Kannada to Sanskrit, and Kannada to Tulu. The datasets for each of the five language pairs were used to train various translation models, including Seq2Seq models such as LSTM, bidirectional LSTM, Conv Seq2Seq, and training state-of-the-art as transformers from scratch, and fine-tuning already pre-trained models. For some models involving monolingual corpora, we implemented backtranslation as well. These models{'} accuracy was later tested with a part of the same dataset using BLEU score as an evaluation metric.",
    selected={true}
}

@inproceedings{tangsali-etal-2022-abstractive,
    title = "Abstractive Approaches To Multidocument Summarization Of Medical Literature Reviews",
    author = "Tangsali, Rahul  and
      Vyawahare, Aditya Jagdish  and
      Mandke, Aditya Vyankatesh  and
      Litake, Onkar Rupesh  and
      Kadam, Dipali Dattatray",
    editor = "Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Lucy Lu",
    booktitle = "Proceedings of the Third Workshop on Scholarly Document Processing",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sdp-1.24",
    pages = "199--203",
    abstract = "Text summarization has been a trending domain of research in NLP in the past few decades. The medical domain is no exception to the same. Medical documents often contain a lot of jargon pertaining to certain domains, and performing an abstractive summarization on the same remains a challenge. This paper presents a summary of the findings that we obtained based on the shared task of Multidocument Summarization for Literature Review (MSLR). We stood fourth in the leaderboards for evaluation on the MS{\^{}}2 and Cochrane datasets. We finetuned pre-trained models such as BART-large, DistilBART and T5-base on both these datasets. These models{'} accuracy was later tested with a part of the same dataset using ROUGE scores as the evaluation metrics.",
}

@misc{malpure2021investigating,
      title={Investigating Transfer Learning Capabilities of Vision Transformers and CNNs by Fine-Tuning a Single Trainable Block}, 
      author={Durvesh Malpure and Onkar Litake and Rajesh Ingle},
      year={2021},
      eprint={2110.05270},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      abstract = "In recent developments in the field of Computer Vision, a rise is seen in the use of transformer-based architectures. They are surpassing the state-of-the-art set by CNN architectures in accuracy but on the other hand, they are computationally very expensive to train from scratch. As these models are quite recent in the Computer Vision field, there is a need to study it's transfer learning capabilities and compare it with CNNs so that we can understand which architecture is better when applied to real world problems with small data. In this work, we follow a simple yet restrictive method for fine-tuning both CNN and Transformer models pretrained on ImageNet1K on CIFAR-10 and compare them with each other. We only unfreeze the last transformer/encoder or last convolutional block of a model and freeze all the layers before it while adding a simple MLP at the end for classification. This simple modification lets us use the raw learned weights of both these neural networks. From our experiments, we find out that transformers-based architectures not only achieve higher accuracy than CNNs but some transformers even achieve this feat with around 4 times lesser number of parameters."
}

@inproceedings{patankar-etal-2022-optimize,
    title = "{O}ptimize{\_}{P}rime@{D}ravidian{L}ang{T}ech-{ACL}2022: Abusive Comment Detection in {T}amil",
    author = "Patankar, Shantanu  and
      Gokhale, Omkar  and
      Litake, Onkar  and
      Mandke, Aditya  and
      Kadam, Dipali",
    editor = "Chakravarthi, Bharathi Raja  and
      Priyadharshini, Ruba  and
      Madasamy, Anand Kumar  and
      Krishnamurthy, Parameswari  and
      Sherly, Elizabeth  and
      Mahesan, Sinnathamby",
    booktitle = "Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dravidianlangtech-1.36",
    doi = "10.18653/v1/2022.dravidianlangtech-1.36",
    pages = "235--239",
    abstract = "This paper tries to address the problem of abusive comment detection in low-resource indic languages. Abusive comments are statements that are offensive to a person or a group of people. These comments are targeted toward individuals belonging to specific ethnicities, genders, caste, race, sexuality, etc. Abusive Comment Detection is a significant problem, especially with the recent rise in social media users. This paper presents the approach used by our team {---} Optimize{\_}Prime, in the ACL 2022 shared task {``}Abusive Comment Detection in Tamil.{''} This task detects and classifies YouTube comments in Tamil and Tamil-English Codemixed format into multiple categories. We have used three methods to optimize our results: Ensemble models, Recurrent Neural Networks, and Transformers. In the Tamil data, MuRIL and XLM-RoBERTA were our best performing models with a macro-averaged f1 score of 0.43. Furthermore, for the Code-mixed data, MuRIL and M-BERT provided sublime results, with a macro-averaged f1 score of 0.45.",
}

@INPROCEEDINGS{10141204,
  author={Sabane, Maithili and Ranade, Aparna and Litake, Onkar and Patil, Parth and Joshi, Raviraj and Kadam, Dipali},
  booktitle={2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={Enhancing Low Resource NER using Assisting Language and Transfer Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1666-1671},
  keywords={Degradation;Adaptation models;Text recognition;Transfer learning;Bit error rate;Learning (artificial intelligence);Benchmark testing;NER;transformers;low resource;transfer learning},
  doi={10.1109/ICAAIC56838.2023.10141204},
  abstract = "Named Entity Recognition (NER) is a fundamental task in NLP that is used to locate the key information in text and is primarily applied in conversational and search systems. In commercial applications, NER or comparable slot filling methods have been widely deployed for popular languages. NER is utilized in applications such as human assets, client benefit, substance classification, and the scholarly community. This research study focuses on identifying name entities for low-resource Indian languages that are closely related, like Hindi and Marathi. This study uses various adaptations of BERT such as baseBERT, AlBERT, and RoBERTa to train a supervised NER model. The, compares multilingual models with monolingual models and establish a baseline. The results show the assisting capabilities of the Hindi and Marathi languages for the NER task. Also, the results show that the models trained using multiple languages perform better than a single language. However, this research study also observe that blind mixing of all datasets doesn't necessarily provide improvements and data selection methods may be required."
  }

@inproceedings{patankar-etal-2022-train,
    title = "To Train or Not to Train: Predicting the Performance of Massively Multilingual Models",
    author = "Patankar, Shantanu  and
      Gokhale, Omkar  and
      Litake, Onkar  and
      Mandke, Aditya  and
      Kadam, Dipali",
    editor = "Ahuja, Kabir  and
      Anastasopoulos, Antonios  and
      Patra, Barun  and
      Neubig, Graham  and
      Choudhury, Monojit  and
      Dandapat, Sandipan  and
      Sitaram, Sunayana  and
      Chaudhary, Vishrav",
    booktitle = "Proceedings of the First Workshop on Scaling Up Multilingual Evaluation",
    month = nov,
    year = "2022",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sumeval-1.2",
    pages = "8--12",
}

@misc{nayak2022suggesting,
      title={Suggesting Relevant Questions for a Query Using Statistical Natural Language Processing Technique}, 
      author={Shriniwas Nayak and Anuj Kanetkar and Hrushabh Hirudkar and Archana Ghotkar and Sheetal Sonawane and Onkar Litake},
      year={2022},
      eprint={2204.12069},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract = "Suggesting similar questions for a user query has many applications ranging from reducing search time of users on e-commerce websites, training of employees in companies to holistic learning for students. The use of Natural Language Processing techniques for suggesting similar questions is prevalent over the existing architecture. Mainly two approaches are studied for finding text similarity namely syntactic and semantic, however each has its draw-backs and fail to provide the desired outcome. In this article, a self-learning combined approach is proposed for determining textual similarity that introduces a robust weighted syntactic and semantic similarity index for determining similar questions from a predetermined database, this approach learns the optimal combination of the mentioned approaches for a database under consideration. Comprehensive analysis has been carried out to justify the efficiency and efficacy of the proposed approach over the existing literature."
}

@article{Mandke_2021,
   title={Analyzing Architectures for Neural Machine Translation using Low Computational Resources},
   volume={10},
   ISSN={2319-4111},
   url={http://dx.doi.org/10.5121/ijnlc.2021.10502},
   DOI={10.5121/ijnlc.2021.10502},
   number={5},
   journal={International Journal on Natural Language Computing},
   publisher={Academy and Industry Research Collaboration Center (AIRCC)},
   author={Mandke, Aditya and Litake, Onkar and Kadam, Dipali},
   year={2021},
   month=oct, pages={9–16},
   abstract = "With the recent developments in the field of Natural Language Processing, there has been a rise in the use of different architectures for Neural Machine Translation. Transformer architectures are used to achieve state-of-the-art accuracy, but they are very computationally expensive to train. Everyone cannot have such setups consisting of high-end GPUs and other resources. We train our models on low computational resources and investigate the results. As expected, transformers outperformed other architectures, but there were some surprising results. Transformers consisting of more encoders and decoders took more time to train but had fewer BLEU scores. LSTM performed well in the experiment and took comparatively less time to train than transformers, making it suitable to use in situations having time constraints."
   }


@misc{litake2024inditext,
      title={IndiText Boost: Text Augmentation for Low Resource India Languages}, 
      author={Onkar Litake and Niraj Yagnik and Shreyas Labhsetwar},
      year={2024},
      eprint={2401.13085},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract = "Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniques surpass LLMs."
}

@article{somayajula2024improving,
  title={Improving long COVID-related text classification: a novel end-to-end domain-adaptive paraphrasing framework},
  author={Somayajula, Sai Ashish and Litake, Onkar and Liang, Youwei and Hosseini, Ramtin and Nemati, Shamim and Wilson, David O and Weinreb, Robert N and Malhotra, Atul and Xie, Pengtao},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={85},
  year={2024},
  publisher={Nature Publishing Group UK London},
  abstract = "The emergence of long COVID during the ongoing COVID-19 pandemic has presented considerable challenges for healthcare professionals and researchers. The task of identifying relevant literature is particularly daunting due to the rapidly evolving scientific landscape, inconsistent definitions, and a lack of standardized nomenclature. This paper proposes a novel solution to this challenge by employing machine learning techniques to classify long COVID literature. However, the scarcity of annotated data for machine learning poses a significant obstacle. To overcome this, we introduce a strategy called medical paraphrasing, which diversifies the training data while maintaining the original content. Additionally, we propose a Data-Reweighting-Based Multi-Level Optimization Framework for Domain Adaptive Paraphrasing, supported by a Meta-Weight-Network (MWN). This innovative approach incorporates feedback from the downstream text classification model to influence the training of the paraphrasing model. During the training process, the framework assigns higher weights to the training examples that contribute more effectively to the downstream task of long COVID text classification. Our findings demonstrate that this method substantially improves the accuracy and efficiency of long COVID literature classification, offering a valuable tool for physicians and researchers navigating this complex and ever-evolving field.",
  selected={true}
}

@misc{sabane2023breaking,
      title={Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi}, 
      author={Maithili Sabane and Onkar Litake and Aman Chadha},
      year={2023},
      eprint={2308.09862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract = "The recent advances in deep-learning have led to the development of highly sophisticated systems with an unquenchable appetite for data. On the other hand, building good deep-learning models for low-resource languages remains a challenging task. This paper focuses on developing a Question Answering dataset for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most spoken language worldwide, with 345 million speakers, and Marathi being the 11th most spoken language globally, with 83.2 million speakers, both languages face limited resources for building efficient Question Answering systems. To tackle the challenge of data scarcity, we have developed a novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples. We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which will facilitate further research in these languages. Leveraging similarity tools, our method holds the potential to create datasets in diverse languages, thereby enhancing the understanding of natural language across varied linguistic contexts. Our fine-tuned models, code, and dataset will be made publicly available."
}

@inproceedings{tangsali-etal-2022-unsupervised,
    title = "Unsupervised and Very-Low Resource Supervised Translation on {G}erman and Sorbian Variant Languages",
    author = "Tangsali, Rahul  and
      Vyawahare, Aditya  and
      Mandke, Aditya  and
      Litake, Onkar  and
      Kadam, Dipali",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.108",
    pages = "1104--1110",
    abstract = "This paper presents the work of team PICT-NLP for the shared task on unsupervised and very low-resource supervised machine translation, organized by the Workshop on Machine Translation, a workshop in collocation with the Conference on Empirical Methods in Natural Language Processing (EMNLP 2022). The paper delineates the approaches we implemented for supervised and unsupervised translation between the following 6 language pairs: German-Lower Sorbian (de-dsb), Lower Sorbian-German (dsb-de), Lower Sorbian-Upper Sorbian (dsb-hsb), Upper Sorbian-Lower Sorbian (hsb-dsb), German-Upper Sorbian (de-hsb), and Upper Sorbian-German (hsb-de). For supervised learning, we implemented the transformer architecture from scratch using the Fairseq library. Whereas for unsupervised learning, we implemented Facebook{'}s XLM masked language modeling approach. We discuss the training details for the models we used, and the results obtained from our approaches. We used the BLEU and chrF metrics for evaluating the accuracies of the generated translations on our systems.",
}

@inproceedings{gokhale-etal-2022-optimize,
    title = "{O}ptimize{\_}{P}rime@{D}ravidian{L}ang{T}ech-{ACL}2022: Emotion Analysis in {T}amil",
    author = "Gokhale, Omkar  and
      Patankar, Shantanu  and
      Litake, Onkar  and
      Mandke, Aditya  and
      Kadam, Dipali",
    editor = "Chakravarthi, Bharathi Raja  and
      Priyadharshini, Ruba  and
      Madasamy, Anand Kumar  and
      Krishnamurthy, Parameswari  and
      Sherly, Elizabeth  and
      Mahesan, Sinnathamby",
    booktitle = "Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dravidianlangtech-1.35",
    doi = "10.18653/v1/2022.dravidianlangtech-1.35",
    pages = "229--234",
    abstract = "This paper aims to perform an emotion analysis of social media comments in Tamil. Emotion analysis is the process of identifying the emotional context of the text. In this paper, we present the findings obtained by Team Optimize{\_}Prime in the ACL 2022 shared task {``}Emotion Analysis in Tamil.{''} The task aimed to classify social media comments into categories of emotion like Joy, Anger, Trust, Disgust, etc. The task was further divided into two subtasks, one with 11 broad categories of emotions and the other with 31 specific categories of emotion. We implemented three different approaches to tackle this problem: transformer-based models, Recurrent Neural Networks (RNNs), and Ensemble models. XLM-RoBERTa performed the best on the first task with a macro-averaged f1 score of 0.27, while MuRIL provided the best results on the second task with a macro-averaged f1 score of 0.13.",
}
